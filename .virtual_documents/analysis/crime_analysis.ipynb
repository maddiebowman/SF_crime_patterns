


# pip install packages not currently downloaded on your device.
!pip install prophet
!pip install pymongo 


# Import dependecies
import pandas as pd
import numpy as np
import requests
import os
import csv
import matplotlib.pyplot as plt
import seaborn as sns
from prophet import Prophet
from prophet.diagnostics import cross_validation, performance_metrics





# Fetch data from API endpoint
url = 'http://127.0.0.1:5000/data'    
response = requests.get(url)
data = response.json()

# Convert JSON data to DataFrame
crime_analysis_df = pd.DataFrame(data)

# Display the column names to verify them
print(crime_analysis_df.columns)


# Show first 5 rows of DataFrame
crime_analysis_df.head(5)


# Count the total incidents for each 'Analysis Neighborhood'
neighborhood_incident_counts = crime_analysis_df['Analysis Neighborhood'].value_counts()

# Display the counts
print(neighborhood_incident_counts)


# Bar chart to show least safe neighborhoods SF

# Plot the bar chart
plt.figure(figsize=(12, 8))
neighborhood_incident_counts.plot(kind='bar')
plt.title('Total Incidents by Analysis Neighborhood')
plt.xlabel('Analysis Neighborhood')
plt.ylabel('Total Incidents')
plt.xticks(rotation=90) 
plt.tight_layout()

# Save as png in images folder
plt.savefig('../images/neighborhood_incidents_bar_chart.png')

# Show the plot
plt.show()


# Find the top 10 neighborhoods with the most incidents
top_neighborhood_crime_counts = crime_analysis_df['Analysis Neighborhood'].value_counts().nlargest(10)


# List of neighborhoods with the most amount of incidents, convert to DF
least_safe_neighborhoods_df = pd.DataFrame(top_neighborhood_crime_counts)

print("Neighborhoods with the most amount of incidents:")
least_safe_neighborhoods_df.head(10)


# Pie chart to show the top 10 neighborhoods with the most incidents

# Plot the pie chart
plt.figure(figsize=(10, 8))
top_neighborhood_crime_counts.plot(kind='pie', autopct='%1.1f%%')
plt.title('Percentage of Incidents by Analysis Neighborhood')
plt.ylabel('')
plt.tight_layout()

# Save as png in images folder
plt.savefig('../images/neighborhood_incidents_pie_chart.png')

# Show the plot
plt.show()


# Find the safest neighborhoods in San Francisco
neighborhoods_least_incidents = neighborhood_incident_counts.nsmallest(10) 

safe_neighborhoods_df = pd.DataFrame(neighborhoods_least_incidents)

# Display the neighborhoods with the least amount of incidents
print("Neighborhoods with the least amount of incidents:")
safe_neighborhoods_df.head(10)


neighborhood_yearly_sum = crime_analysis_df.groupby(["Analysis Neighborhood", "Incident Year"]).size().unstack(fill_value=0)

# Ensure column names are integers
neighborhood_yearly_sum.columns = neighborhood_yearly_sum.columns.astype(int)

# Check if 2018 is a column
if 2018 not in neighborhood_yearly_sum.columns:
    print("Year 2018 is not in the columns")
else:
    neighborhood_yearly_sum_sorted = neighborhood_yearly_sum.sort_values(by=2018, ascending=False)
    print(neighborhood_yearly_sum_sorted)


top_10_neighborhoods = neighborhood_yearly_sum_sorted.head(10)

incident_years = top_10_neighborhoods.columns

for neighborhood in top_10_neighborhoods.index:
    plt.plot(incident_years, top_10_neighborhoods.loc[neighborhood], marker='o', linestyle='-', label=neighborhood)

plt.xlabel("Incident Year")
plt.ylabel("the number of incidents")
plt.title("Annual Change in the number of incidents by neighborhood")
plt.grid(True)
plt.legend(ncol=2,bbox_to_anchor=(1, -0.1))
plt.show()





# Count crime occurance grouped by category and year
count_category_df = crime_analysis_df.groupby(["Incident Year", "Incident Category"]).size().unstack(fill_value=0)
count_category_df


# Find the sum for each category 
count_category_df.sum()


# Reformat for stacking
switch_df=count_category_df.T
switch_df


switch_df.sum()


bottom = None
colors = plt.cm.tab20.colors

for i,col in enumerate(count_category_df.columns[0:]):
    bar=plt.bar(count_category_df.index,count_category_df[col],label =col, bottom=bottom,color=colors[i])
    if bottom is None:
        bottom = count_category_df[col]
    else:
        bottom += count_category_df[col]
        
plt.xlabel("Incident Year")
plt.ylabel("Incident Categories")

plt.title("Annual Counts of Incident Categories in SF")
plt.legend(ncol=2,bbox_to_anchor=(1.2, -0.1))
plt.figure(figsize=(18, 12))
# plt.savefig("../images/Trends_in_incidents.png",bbox_inches='tight',transparent=True)

plt.show()





# Load the data from the heatmap
data = pd.read_csv('../data/data_hm.csv')
datetime_df = pd.DataFrame(data)

# Display DataFrame
datetime_df.head(5) 


# Display basic statistics
stats = datetime_df.describe()
stats


min_val = 198
max_val = 507

# Calculate the interquartile range
range_val = max_val - min_val

# Quartiles
q1 = min_val + range_val * 0.25
q2 = min_val + range_val * 0.50
q3 = min_val + range_val * 0.75

print("Low Range:", min_val, "to", q1)
print("Medium Range:", q1, "to", q2)
print("High Range:", q2, "to", q3)
print("Extreme Range:", q3, "to", max_val)


# Plot a heatmap with seaborn

datetime_df.set_index('category', inplace=True)

# Plotting the heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(datetime_df, annot=True, cmap='coolwarm', fmt="d", linewidths=.5)
plt.title('Heatmap of Activity by Day and Time')
plt.xlabel('Day of the Week')
plt.ylabel('Time of Day')
plt.savefig("../images/heatmap_sb")


# Create box plot for analysis of crime by day of week
datetime_df = pd.DataFrame(data)
datetime_df.set_index('category', inplace=True)

# Convert the DataFrame from wide to long format
df_long = datetime_df.reset_index().melt(id_vars='category', var_name='Day', value_name='Incidents')

# Plotting the box plot
plt.figure(figsize=(12, 8))
sns.boxplot(x='Day', y='Incidents',data=df_long)
plt.title('Box Plot of Incidents by Day of the Week')
plt.xlabel('Day of the Week')
plt.ylabel('Number of Incidents')
plt.savefig("../images/incidents_day_week")



# Create box plot for analysis of crime by time of day
datetime_df = pd.DataFrame(data)

# Step 1: Melt the DataFrame to long format
df_long = datetime_df.melt(id_vars='category', var_name='Day', value_name='Incidents')

# Step 2: Aggregate data by category, ignoring the specific days
aggregated_data = df_long.groupby('category')['Incidents'].apply(list).reset_index()

# Step 3: Explode the list to separate rows for plotting
exploded_data = aggregated_data.explode('Incidents')

# Convert 'Incidents' to numeric (if necessary)
exploded_data['Incidents'] = pd.to_numeric(exploded_data['Incidents'])

# Step 4: Plotting the box plot
plt.figure(figsize=(10, 6))
sns.boxplot(x='category', y='Incidents', data=exploded_data)
plt.title('Box Plot of Incidents by Time of Day')
plt.xlabel('Time of Day')
plt.ylabel('Number of Incidents')
plt.savefig("../images/incidents_timeofday.png")
plt.show()






# Fetch data from API endpoint
url = 'http://127.0.0.1:5000/data'    
response = requests.get(url)
data = response.json()

# Convert JSON data to DataFrame
df_forecast = pd.DataFrame(data)

# Display the column names to verify them
print(df_forecast.columns)


len(df_forecast)


# Data Preprocessing and Aggregation
# Convert columns to appropriate data types
df_forecast['Incident Year'] = df_forecast['Incident Year'].astype(int)
df_forecast['Incident Date'] = pd.to_datetime(df_forecast['Incident Date'], errors='coerce')
df_forecast['Incident Category'] = df_forecast['Incident Category'].astype(str)

# Filter data starting from January 2018
df_forecast = df_forecast[df_forecast['Incident Date'] >= '2018-01-01']

# Check for and handle missing or incorrect dates
df_forecast = df_forecast.dropna(subset=['Incident Date'])

# Create a 'date' column
df_forecast['date'] = df_forecast['Incident Date'].dt.to_period('M').dt.to_timestamp()

# Aggregate data by date and incident category, summing the counts
aggregated_data = df_forecast.groupby(['date', 'Incident Category']).size().reset_index(name='count')

def forecast_category(aggregated_data, category):
    category_data = aggregated_data[aggregated_data['Incident Category'] == category]
    ts_data = category_data[['date', 'count']].rename(columns={'date': 'ds', 'count': 'y'})

    # Check if there is enough data (at least 12 data points)
    if len(ts_data) < 12:
        print(f"Not enough data to model category: {category}")
        return

    model = Prophet()
    model.fit(ts_data)
    future = model.make_future_dataframe(periods=24, freq='M')
    forecast = model.predict(future)

    plt.figure(figsize=(12, 6))
    plt.plot(ts_data['ds'], ts_data['y'], label='Observed')
    plt.plot(forecast['ds'], forecast['yhat'], label='Forecast', color='r')
    plt.fill_between(forecast['ds'], forecast['yhat_lower'], forecast['yhat_upper'], color='pink', alpha=0.3)
    plt.title(f'{category} - Historical and Forecasted Data')
    plt.xlabel('Year')
    plt.ylabel('Incident Count')
    plt.legend()
    plt.savefig(f'../images/forecasting/{category}_forecast.png')
    plt.show()
    
    return model

def evaluate_model(model, ts_data):
    # Perform cross-validation
    df_cv = cross_validation(model, initial='366 days', period='180 days', horizon='730 days')
    
    # Calculate performance metrics
    df_performance = performance_metrics(df_cv)
    
    print(df_performance[['horizon', 'mae', 'mse', 'mape']])
    
    return df_cv, df_performance

# Example usage
# Identify all unique incident categories
categories = aggregated_data['Incident Category'].unique()

for category in categories:
    model = forecast_category(aggregated_data, category)
    if model is not None:
        ts_data = aggregated_data[aggregated_data['Incident Category'] == category][['date', 'count']].rename(columns={'date': 'ds', 'count': 'y'})
        df_cv, df_performance = evaluate_model(model, ts_data)
